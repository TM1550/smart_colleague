from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import json
import logging
import requests
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from pathlib import Path
import pickle
import hashlib
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ==================== Data Models ====================


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    description: str
    instruction: str
    user_query: str
    status: str  
    search_time_ms: float = 0.0
    error_message: Optional[str] = None
    similarity_score: Optional[float] = None  # –ù–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä—å"""
        result = {
            "description": self.description,
            "instruction": self.instruction,
            "user_query": self.user_query,
            "status": self.status,
            "search_time_ms": round(self.search_time_ms, 2),
            "error_message": self.error_message
        }
        if self.similarity_score is not None:
            result["similarity_score"] = round(self.similarity_score, 4)
        return result


# ==================== Vector Store ====================

class VectorStore:
    """–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"""
    
    def __init__(self, 
                 model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                 use_cosine: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            use_cosine: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (True) –∏–ª–∏ –µ–≤–∫–ª–∏–¥–æ–≤–æ (False)
        """
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.instruction_texts = []  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        self.instruction_metadata = []  # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        self.use_cosine = use_cosine
        
    def _instruction_to_text(self, instruction: Dict[str, Any]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        task_name = instruction.get("task_name", "")
        instruction_text = instruction.get("instruction", "")
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –ø–æ–ª—è –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        return f"{task_name}. {instruction_text}"
    
    def build_index(self, instructions: List[Dict[str, Any]]):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        """
        if not instructions:
            logger.warning("–ù–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.instruction_texts = []
        self.instruction_metadata = []
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        texts_for_embedding = []
        for i, instr in enumerate(instructions):
            text = self._instruction_to_text(instr)
            texts_for_embedding.append(text)
            self.instruction_texts.append(text)
            self.instruction_metadata.append(instr)
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts_for_embedding)} –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π...")
        embeddings = self.model.encode(texts_for_embedding, 
                                      convert_to_numpy=True, 
                                      show_progress_bar=True)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        if self.use_cosine:
            faiss.normalize_L2(embeddings)
        
        # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        dimension = embeddings.shape[1]
        
        if len(embeddings) > 10000:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º IVF –∏–Ω–¥–µ–∫—Å
            nlist = min(100, len(embeddings) // 39)
            quantizer = faiss.IndexFlatL2(dimension)
            self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            self.index.train(embeddings.astype('float32'))
            self.index.add(embeddings.astype('float32'))
            self.index.nprobe = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∏–∂–∞–π—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        else:
            # –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
            if self.use_cosine:
                self.index = faiss.IndexFlatIP(dimension)  # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            else:
                self.index = faiss.IndexFlatL2(dimension)  # –ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        
        self.index.add(embeddings.astype('float32'))
        logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension}")
    
    def search_similar(self, query: str, k: int = 10) -> List[Tuple[int, float]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∏–Ω–¥–µ–∫—Å, —Å—Ö–æ–¥—Å—Ç–≤–æ)
        """
        if self.index is None or len(self.instruction_texts) == 0:
            logger.warning("–í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
            return []
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        if self.use_cosine:
            faiss.normalize_L2(query_embedding)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        distances, indices = self.index.search(query_embedding.astype('float32'), min(k, len(self.instruction_texts)))
        
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx >= 0 and idx < len(self.instruction_texts):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ —Å—Ö–æ–∂–µ—Å—Ç—å
                if self.use_cosine:
                    similarity = distance  # –î–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —ç—Ç–æ —É–∂–µ —Å—Ö–æ–∂–µ—Å—Ç—å
                else:
                    similarity = 1 / (1 + distance)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ —Å—Ö–æ–∂–µ—Å—Ç—å
                
                if similarity > 0:  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
                    results.append((idx, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        return results
    
    def get_instruction_by_index(self, idx: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        if 0 <= idx < len(self.instruction_metadata):
            return self.instruction_metadata[idx]
        return None
    
    def save_index(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫"""
        if self.index:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            data = {
                'instruction_texts': self.instruction_texts,
                'instruction_metadata': self.instruction_metadata,
                'use_cosine': self.use_cosine
            }
            
            with open(f"{path}_data.pkl", 'wb') as f:
                pickle.dump(data, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            faiss.write_index(self.index, f"{path}_index.faiss")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (—Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–µ–π)
            model_info = {
                'model_name': self.model[0].auto_model.config._name_or_path,
                'use_cosine': self.use_cosine
            }
            with open(f"{path}_model.pkl", 'wb') as f:
                pickle.dump(model_info, f)
            
            logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {path}")
    
    def load_index(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            with open(f"{path}_data.pkl", 'rb') as f:
                data = pickle.load(f)
                self.instruction_texts = data['instruction_texts']
                self.instruction_metadata = data['instruction_metadata']
                self.use_cosine = data.get('use_cosine', True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            self.index = faiss.read_index(f"{path}_index.faiss")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            with open(f"{path}_model.pkl", 'rb') as f:
                model_info = pickle.load(f)
                # –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
                self.model = SentenceTransformer(model_info['model_name'])
            
            logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {path}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")


# ==================== API Client ====================

class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM API"""
    
    def __init__(
        self,
        api_key: str,
        model: str = "tngtech/deepseek-r1t2-chimera:free",
        base_url: str = "https://openrouter.ai/api/v1/chat/completions",
        timeout: int = 120
    ):
        self.api_key: str = api_key
        self.base_url: str = base_url
        self.model: str = model
        self.timeout: int = timeout
            
    def call_api(self, messages: List[Dict[str, str]], temperature: float = 0.7) -> str:
        """–ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –≤—ã–∑–æ–≤ API"""
        
        payload: Dict[str, Any] = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
        }
        
        headers: Dict[str, str] = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        try:
            response: requests.Response = requests.post(
                self.base_url,
                headers=headers,
                json=payload,
                timeout=self.timeout,
            )
            response.raise_for_status()
            
            data = response.json()
            return data["choices"][0]["message"]["content"].strip()
        
        except requests.exceptions.Timeout:
            logger.error("‚ùå API request timed out")
            raise RuntimeError("API request timed out")
        
        except requests.exceptions.HTTPError as e:
            logger.error(f"‚ùå HTTP Error: {e.response.status_code}")
            raise RuntimeError(f"HTTP Error: {e.response.status_code}")
        
        except Exception as e:
            logger.error(f"‚ùå API error: {e}")
            raise RuntimeError(f"API error: {str(e)}")


# ==================== Instruction Search Engine ====================

class InstructionSearchEngine:
    """–ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self, 
                 llm_client: LLMClient,
                 vector_model: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                 use_vector_search: bool = True,
                 similarity_threshold: float = 0.3):
        self.llm_client: LLMClient = llm_client
        self.vector_store = VectorStore(model_name=vector_model) if use_vector_search else None
        self.use_vector_search = use_vector_search
        self.similarity_threshold = similarity_threshold
    
    def build_vector_index(self, instructions: List[Dict[str, Any]]):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        if self.vector_store:
            self.vector_store.build_index(instructions)
    
    def vector_search_candidates(self, user_query: str, top_k: int = 20) -> List[Tuple[Dict[str, Any], float]]:
        """
        –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, —Å—Ö–æ–∂–µ—Å—Ç—å)
        """
        if not self.vector_store or not self.use_vector_search:
            return []
        
        candidates = []
        vector_results = self.vector_store.search_similar(user_query, k=top_k)
        
        for idx, similarity in vector_results:
            if similarity >= self.similarity_threshold:
                instruction = self.vector_store.get_instruction_by_index(idx)
                if instruction:
                    candidates.append((instruction, similarity))
        
        logger.info(f"üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–ø–æ—Ä–æ–≥: {self.similarity_threshold})")
        return candidates
    
    def _extract_relevance_score(self, response_text: str) -> float:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                score = data.get("relevance_score", 0.0)
                return float(score) / 100.0 if score > 1 else float(score)
        except:
            pass
        
        # Fallback: –∏—â–µ—Ç —á–∏—Å–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        import re
        scores = re.findall(r'\b(0\.\d+|[0-9]+)\b', response_text)
        if scores:
            try:
                score = float(scores[0])
                return score / 100.0 if score > 1 else score
            except:
                pass
        
        return 0.5  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    
    def evaluate_instruction_relevance(
        self,
        user_query: str,
        instruction: str,
    ) -> tuple[float, str, str, str]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            tuple: (score, reasoning, instruction, description)
        """
        prompt = f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –û—Ü–µ–Ω–∏, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_query}"

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {instruction}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π:
1. –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –∑–∞–¥–∞—á–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è?
2. –ï—Å—Ç—å –ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –∏ –∑–∞–ø—Ä–æ—Å–æ–º?
3. –ü–æ–º–æ–∂–µ—Ç –ª–∏ —ç—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Ä–µ—à–∏—Ç—å –µ–≥–æ –∑–∞–¥–∞—á—É?

–û—Ç–≤–µ—Ç—å JSON-–æ–±—ä–µ–∫—Ç–æ–º:
{{
  "relevance_score": <—á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 1>,
  "instruction": <–ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞–ø–∏—Å–∞–Ω–Ω–∞—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è>,
  "reasoning": "<–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è>",
  "description": "<–ø–æ—à–∞–≥–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –ø–æ–Ω—è—Ç–Ω–æ–µ –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ>"
}}

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ JSON, –±–µ–∑ –¥—Ä—É–≥–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤):"""
        
        try:
            logger.info(f"–û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞: '{user_query[:50]}...'")
            
            response = self.llm_client.call_api(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            
            if json_match:
                try:
                    data = json.loads(json_match.group())
                    score = float(data.get("relevance_score", 0.5))
                    found_instruction = str(data.get("instruction", ""))
                    description = str(data.get("description", ""))
                    reasoning = str(data.get("reasoning", "–ù–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"))
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score
                    score = max(0.0, min(1.0, score))
                    
                    logger.info(f"  ‚úì Score: {score:.2f}, Reasoning: {reasoning[:50]}...")
                    return score, reasoning, found_instruction, description
                
                except json.JSONDecodeError:
                    logger.warning(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞")
                    return 0.5, "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ API", "", ""
            else:
                logger.warning(f"  ‚ö†Ô∏è JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ")
                return 0.5, "API –≤–µ—Ä–Ω—É–ª –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç", "", ""
        
        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            return 0.0, f"–û—à–∏–±–∫–∞: {str(e)}", "", ""

    def search_hybrid(
        self,
        user_query: str,
        instructions: List[Dict[str, Any]],
        vector_top_k: int = 10,
        llm_top_k: int = 3,
        min_relevance: float = 0.2
    ) -> SearchResult:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–Ω–∞—á–∞–ª–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π, –∑–∞—Ç–µ–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ LLM
        
        Args:
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            instructions: –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å)
            vector_top_k: –°–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏—Å–∫–∞—Ç—å —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            llm_top_k: –°–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —á–µ—Ä–µ–∑ LLM
            min_relevance: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        
        Returns:
            SearchResult —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
        """
        import time
        start_time = time.time()
        
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{user_query}'")
        
        # –®–∞–≥ 1: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = []
        if self.use_vector_search:
            candidates = self.vector_search_candidates(user_query, top_k=vector_top_k)
        else:
            # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            candidates = [(instr, 0.5) for instr in instructions]
        
        if not candidates:
            logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
            search_time = (time.time() - start_time) * 1000
            return SearchResult(
                description="",
                instruction="",
                user_query=user_query,
                status="no_matches",
                search_time_ms=search_time,
                error_message="–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"
            )
        
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
        
        # –®–∞–≥ 2: –û—Ü–µ–Ω–∫–∞ —Ç–æ–ø-N –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM
        best_score = 0.0
        best_instruction = ""
        best_description = ""
        best_reasoning = ""
        best_similarity = 0.0
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM
        candidates_to_evaluate = candidates[:min(llm_top_k, len(candidates))]
        
        for candidate, similarity in candidates_to_evaluate:
            instruction_text = self._instruction_to_str(candidate)
            score, reasoning, found_instr, description = self.evaluate_instruction_relevance(
                user_query, instruction_text
            )
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: —É—á–∏—Ç—ã–≤–∞–µ–º –∏ –≤–µ–∫—Ç–æ—Ä–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å, –∏ –æ—Ü–µ–Ω–∫—É LLM
            combined_score = (similarity * 0.4) + (score * 0.6)
            
            if combined_score > best_score and score >= min_relevance:
                best_score = combined_score
                best_similarity = similarity
                best_instruction = found_instr
                best_description = description
                best_reasoning = reasoning
        
        search_time = (time.time() - start_time) * 1000
        
        if best_score < min_relevance:
            logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π")
            return SearchResult(
                description="",
                instruction="",
                user_query=user_query,
                status="no_matches",
                search_time_ms=search_time,
                error_message=f"–õ—É—á—à–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ ({best_score:.2f}) –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ ({min_relevance})"
            )
        
        logger.info(f"‚úÖ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –õ—É—á—à–∞—è –æ—Ü–µ–Ω–∫–∞: {best_score:.2f} (vector: {best_similarity:.2f})")
        
        return SearchResult(
            description=best_description,
            instruction=best_instruction,
            user_query=user_query,
            status="success",
            search_time_ms=search_time,
            similarity_score=best_score
        )
    
    def _instruction_to_str(self, instruction: Dict[str, Any]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
        task_name = instruction.get("task_name", "")
        full_path = instruction.get("full_path", "")
        instruction_text = instruction.get("instruction", "")
        
        return f"–ó–∞–¥–∞—á–∞: {task_name}\n–ü—É—Ç—å: {full_path}\n–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\n{instruction_text}"
    
    def search(
        self,
        user_query: str,
        instructions: List[Dict[str, Any]],
    ) -> SearchResult:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
        """
        return self.search_hybrid(user_query, instructions)


# ==================== Question Processor ====================

class QuestionProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    
    def __init__(self, llm_client: LLMClient, search_engine: InstructionSearchEngine):
        self.llm_client: LLMClient = llm_client
        self.search_engine: InstructionSearchEngine = search_engine
    
    def generate_recommendation(
        self,
        user_query: str,
        search_result: SearchResult
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
        Args:
            user_query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            search_result: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        
        Returns:
            –¢–µ–∫—Å—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        
        if search_result.status == "no_matches":
            return f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: '{user_query}'. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –≥–ª–∞–≤–Ω–æ–º—É –º–µ–Ω—é."
        
        top_match = search_result.instruction
        
        if not top_match:
            return "–û—à–∏–±–∫–∞: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"
        
        prompt = f"""–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π —Å–æ–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_query}"

–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:
- –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {top_match}
- –û–ø–∏—Å–∞–Ω–∏–µ: {search_result.description}

–ù–∞–ø–∏—à–∏ –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
1. –ü–æ–¥—Ç–≤–µ—Ä–¥–∏, —á—Ç–æ —Ç—ã –ø–æ–Ω—è–ª –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
2. –ü—Ä–µ–¥–ª–æ–∂–∏ –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
3. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –¥–∞–π –∫—Ä–∞—Ç–∫–∏–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è
4. –ë—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º

–û—Ç–≤–µ—Ç (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω):"""
        
        try:
            recommendation = self.llm_client.call_api(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            return recommendation
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}")
            # Fallback –æ—Ç–≤–µ—Ç
            return f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é: {top_match[:200]}..."


# ==================== Main Interface ====================

class InstructionAssistant:
    """–ì–ª–∞–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º"""
    
    def __init__(self, 
                 api_key: str,
                 use_vector_search: bool = True,
                 vector_model: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        self.llm_client = LLMClient(api_key=api_key)
        self.search_engine = InstructionSearchEngine(
            llm_client=self.llm_client,
            use_vector_search=use_vector_search,
            vector_model=vector_model
        )
        self.question_processor = QuestionProcessor(
            llm_client=self.llm_client,
            search_engine=self.search_engine
        )
        self.current_instructions: List[Dict[str, Any]] = []
        self.use_vector_search = use_vector_search
    
    def load_instructions(self, instructions: List[Dict[str, Any]]) -> None:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        
        Args:
            instructions: –°–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        """
        self.current_instructions = instructions
        
        if self.use_vector_search:
            logger.info(f"üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(instructions)} –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π...")
            self.search_engine.build_vector_index(instructions)
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(instructions)} –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π")
    
    def save_vector_index(self, path: str = "./vector_index"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫"""
        if self.use_vector_search and self.search_engine.vector_store:
            self.search_engine.vector_store.save_index(path)
    
    def load_vector_index(self, path: str = "./vector_index"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å –¥–∏—Å–∫–∞"""
        if self.use_vector_search and self.search_engine.vector_store:
            self.search_engine.vector_store.load_index(path)
    
    def answer_question(
        self,
        user_query: str,
        vector_top_k: int = 15,
        llm_top_k: int = 3,
        min_relevance: float = 0.3,
        include_recommendation: bool = True
    ) -> Dict[str, Any]:
        """
        –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
        Args:
            user_query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            vector_top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            llm_top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM
            min_relevance: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            include_recommendation: –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        
        if not self.current_instructions:
            return {
                "status": "error",
                "error_message": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load_instructions()."
            }
        
        logger.info(f"\n{'='*60}")
        logger.info(f"üí¨ –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: '{user_query}'")
        logger.info(f"{'='*60}")
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        search_result = self.search_engine.search_hybrid(
            user_query=user_query,
            instructions=self.current_instructions,
            vector_top_k=vector_top_k,
            llm_top_k=llm_top_k,
            min_relevance=min_relevance
        )
        
        result_dict = search_result.to_dict()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if include_recommendation and search_result.status == "success":
            recommendation = self.question_processor.generate_recommendation(
                user_query=user_query,
                search_result=search_result
            )
            result_dict["recommendation"] = recommendation
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {search_result.search_time_ms:.0f}–º—Å")
        
        return result_dict